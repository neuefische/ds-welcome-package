{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To open this notebook in Google Colab and start coding, click on the Colab icon below.\n",
    "\n",
    "<table style=\"border:2px solid orange\" align=\"left\">\n",
    "  <td style=\"border:2px solid orange \">\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/neuefische/ds-welcome-package/blob/main/statistics/2-Probability.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Introduction to Probability\n",
    "In this notebook you will find the statistics basics that we require for the bootcamp.\n",
    "If you still have problems with one or the other term, we strongly recommend you to work on it a bit more. \n",
    "\n",
    "<img src=\"https://i.pinimg.com/originals/36/c0/9a/36c09a6529b0b10212c9a96c5f243a4e.jpg\"\n",
    "     alt=\"Alice through the looking glass\"\n",
    "     style=\"float: left; margin-right: 10px; height: 400px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability\n",
    "\n",
    "Probability is the branch of mathematics concerning numerical descriptions of how likely an event is to occur, or how likely it is that a proposition is true. The probability of an event is a number between 0 and 1, where, roughly speaking, 0 indicates impossibility of the event and 1 indicates certainty. The higher the probability of an event, the more likely it is that the event will occur. A simple example is the tossing of a fair (unbiased) coin. Since the coin is fair, the two outcomes (\"heads\" and \"tails\") are both equally probable; the probability of \"heads\" equals the probability of \"tails\"; and since no other outcomes are possible, the probability of either \"heads\" or \"tails\" is 1/2 (which could also be written as 0.5 or 50%).\n",
    "\n",
    "These concepts have been given an axiomatic mathematical formalization in probability theory, which is used widely in areas of study such as statistics, mathematics, science, finance, gambling, artificial intelligence, machine learning, computer science, game theory, and philosophy to, for example, draw inferences about the expected frequency of events. Probability theory is also used to describe the underlying mechanics and regularities of complex systems.\n",
    "\n",
    "Consider an experiment that can produce a number of results. The collection of all possible results is called the sample space of the experiment, sometimes denoted as $\\Omega$. The power set of the sample space is formed by considering all different collections of possible results. For example, rolling a die can produce six possible results. One collection of possible results gives an odd number on the die. Thus, the subset {1,3,5} is an element of the power set of the sample space of dice rolls. These collections are called \"events\". In this case, {1,3,5} is the event that the die falls on some odd number. If the results that actually occur fall in a given event, the event is said to have occurred.\n",
    "\n",
    "A probability is a way of assigning every event a value between zero and one, with the requirement that the event made up of all possible results (in our example, the event {1,2,3,4,5,6}) is assigned a value of one. To qualify as a probability, the assignment of values must satisfy the requirement that for any collection of mutually exclusive events (events with no common results, such as the events {1,6}, {3}, and {2,4}), the probability that at least one of the events will occur is given by the sum of the probabilities of all the individual events.\n",
    "\n",
    "The probability of an event A is written as $P(A),p(A)$, or $Pr(A)$. This mathematical definition of probability can extend to infinite sample spaces, and even uncountable sample spaces, using the concept of a measure.\n",
    "\n",
    "The opposite or complement of an event A is the event [not A] (that is, the event of A not occurring), often denoted as $ A',A^{c}, \\overline {A},A^{\\complement },\\neg A$; its probability is given by P(not A) = 1 − P(A). As an example, the chance of not rolling a six on a six-sided die is $1 - (\\textrm {chance of rolling a six}) =1-{\\tfrac {1}{6}}= \\tfrac {5}{6}$.\n",
    "\n",
    "If two events A and B occur on a single performance of an experiment, this is called the intersection or joint probability of A and B, denoted as $P(A\\cap B)$.\n",
    "<br>\n",
    "<br>\n",
    "**Independent events**\n",
    "<br>\n",
    "If two events, A and B are independent then the joint probability is:\n",
    "\n",
    "$$P(A{\\mbox{ and }}B)=P(A\\cap B)=P(A)P(B)$$\n",
    "\n",
    "For example, if two coins are flipped, then the chance of both being heads is $\\tfrac {1}{2} \\times \\tfrac {1}{2}=\\tfrac {1}{4}$.\n",
    "<br>\n",
    "![](http://blog.jzhanson.com/assets/dl-part-1/conditional-2.png)\n",
    "Source:[http://blog.jzhanson.com/blog/dl/tutorial/2017/12/30/dl-1.html](http://blog.jzhanson.com/blog/dl/tutorial/2017/12/30/dl-1.html)\n",
    "<br>\n",
    "<br>\n",
    "**Mutually exclusive events**\n",
    "\n",
    "If either event A or event B can occur but never both simultaneously, then they are called mutually exclusive events.\n",
    "\n",
    "If two events are mutually exclusive, then the probability of **both** occurring is denoted as $P(A\\cap B)$ and\n",
    "\n",
    "$$ P(A{\\mbox{ and }B)=P(A\\cap B)=0}$$\n",
    "\n",
    "If two events are mutually exclusive, then the probability of **either** occurring is denoted as $P(A\\cup B)$ and\n",
    "\n",
    "$$P(A\\mbox { or } B) = P(A\\cup B)=P(A)+P(B)-P(A\\cap B)=P(A)+P(B)-0=P(A)+P(B)$$\n",
    "\n",
    "For example, the chance of rolling a 1 or 2 on a six-sided die is $ P(1{\\mbox{ or }}2)=P(1)+P(2)={\\tfrac {1}{6}}+{\\tfrac {1}{6}}={\\tfrac {1}{3}}.$\n",
    "\n",
    "**Not mutually exclusive events**\n",
    "\n",
    "If the events are not mutually exclusive then\n",
    "\n",
    "$$ P\\left(A{\\hbox{ or }}B\\right)=P(A\\cup B)=P\\left(A\\right)+P\\left(B\\right)-P\\left(A{\\mbox{ and }}B\\right).$$\n",
    "\n",
    "For example, when drawing a single card at random from a regular deck of cards, the chance of getting a heart or a face card (J,Q,K) (or one that is both) is $\\tfrac {13}{52}+\\tfrac {12}{52}-\\tfrac {3}{52}=\\tfrac {11}{26}$, since among the 52 cards of a deck, 13 are hearts, 12 are face cards, and 3 are both: here the possibilities included in the \"3 that are both\" are included in each of the \"13 hearts\" and the \"12 face cards\", but should only be counted once.\n",
    "<br>\n",
    "<br>\n",
    "**Conditional probability**\n",
    "\n",
    "Conditional probability is the probability of some event A, given the occurrence of some other event B. Conditional probability is written $P(A\\mid B)$, and is read \"the probability of A, given B\". It is defined by\n",
    "\n",
    "$$ P(A\\mid B)={\\frac {P(A\\cap B)}{P(B)}}.$$\n",
    "If $P(B)=0$ then $P(A\\mid B)$ is formally undefined by this expression. However, it is possible to define a conditional probability for some zero-probability events using a σ-algebra of such events (such as those arising from a continuous random variable).\n",
    "\n",
    "For example, in a bag of 2 red balls and 2 blue balls (4 balls in total), the probability of taking a red ball is $\\frac {1}{2}$; however, when taking a second ball, the probability of it being either a red ball or a blue ball depends on the ball previously taken. For example, if a red ball was taken, then the probability of picking a red ball again would be $\\frac {1}{3}$, since only 1 red and 2 blue balls would have been remaining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes’ Theorem\n",
    "\n",
    "Bayes' theorem is a formula that describes how to update the probabilities of hypotheses when given evidence. It follows simply from the axioms of conditional probability, but can be used to powerfully reason about a wide range of problems involving belief updates.\n",
    "\n",
    "Given a hypothesis HH and evidence EE, Bayes' theorem states that the relationship between the probability of the hypothesis before getting the evidence $P(H)P(H)$ and the probability of the hypothesis after getting the evidence $P(H \\mid E)P(H∣E)$ is\n",
    "\n",
    "$$P(H \\mid E)= \\frac {P(E \\mid H)}{P(E)}P(H).$$\n",
    "\n",
    "Many modern machine learning techniques rely on Bayes' theorem. For instance, spam filters use Bayesian updating to determine whether an email is real or spam, given the words in the email. Additionally, many specific techniques in statistics, such as calculating pp-values or interpreting medical results, are best described in terms of how they contribute to updating hypotheses using Bayes' theorem.\n",
    "\n",
    "Bayes' theorem centers on relating different conditional probabilities. A conditional probability is an expression of how probable one event is given that some other event occurred (a fixed value). For instance, \"what is the probability that the sidewalk is wet?\" will have a different answer than \"what is the probability that the sidewalk is wet given that it rained earlier?\"\n",
    "\n",
    "**Bayes' Theorem**\n",
    "\n",
    "$$ P(A \\mid B) = \\frac{P(B \\mid A)} {P(B)} P(A)$$\n",
    "\n",
    "While this is an equation that applies to any probability distribution over events AA and BB, it has a particularly nice interpretation in the case where AA represents a hypothesis HH and BB represents some observed evidence EE. In this case, the formula can be written as\n",
    "\n",
    "$$P(H \\mid E) = \\frac{P(E \\mid H)}{P(E)} P(H).$$\n",
    "\n",
    "\n",
    "This relates the probability of the hypothesis before getting the evidence P(H)P(H), to the probability of the hypothesis after getting the evidence, $P(H \\mid E)P(H∣E)$. For this reason, P(H)P(H) is called the prior probability, while $P(H \\mid E)P(H∣E)$ is called the posterior probability. The factor that relates the two, $\\frac{P(E \\mid H)}{P(E)}$, is called the likelihood ratio. Using these terms, Bayes' theorem can be rephrased as \"the posterior probability equals the prior probability times the likelihood ratio.\"\n",
    "\n",
    "**Example**\n",
    "\n",
    "If a single card is drawn from a standard deck of playing cards, the probability that the card is a king is 4/52, since there are 4 kings in a standard deck of 52 cards. Rewording this, if $King$ is the event \"this card is a king,\" the prior probability $P(\\text{King}) = \\frac{4}{52} = \\frac{1}{13}$.\n",
    "\n",
    "If evidence is provided (for instance, someone looks at the card) that the single card is a face card, then the posterior probability $P(\\text{King} \\mid \\text{Face})$ can be calculated using Bayes' theorem:\n",
    "\n",
    "$$P(\\text{King} \\mid \\text{Face}) = \\frac{P(\\text{Face} \\mid \\text{King})}{P(\\text{Face})} P(\\text{King}).$$\n",
    "\n",
    "Since every King is also a face card, $P(\\text{Face} \\mid \\text{King}) = 1$. Since there are 3 face cards in each suit (Jack, Queen, King) , the probability of a face card is $P(\\text{Face}) = \\frac{3}{13}$. Combining these gives a likelihood ratio of $\\frac{1}{\\hspace{2mm} \\frac3{13}\\hspace{2mm} } = \\frac{13}{3}$. \n",
    "\n",
    "Using Bayes' theorem gives $P(\\text{King} \\mid \\text{Face}) = \\frac{13}{3} \\frac{1}{13} = \\frac{1}{3}$.\n",
    "\n",
    "\n",
    "**Why is the Bayes' Theorem important for DataScience?**\n",
    "\n",
    "In [this article](https://towardsdatascience.com/bayes-theorem-the-holy-grail-of-data-science-55d93315defb) you can read more about it!\n",
    "\n",
    "\"In finance, for example, Bayes’ theorem can be used to rate the risk of lending money to potential borrowers. In medicine, the theorem can be used to determine the accuracy of medical test results by taking into consideration how likely any given person is to have a disease and the general accuracy of the test. ... **A hard-to-compute probability distribution is represented by probabilities that are very easy to calculate.**\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Independence, Law of total Probability and Chain Rule of Probability\n",
    "\n",
    "### Statistical Independence\n",
    "\n",
    "Statistical independence is a concept in probability theory. Two events A and B are statistical independent if and only if their joint probability can be factorized into their marginal probabilities, i.e., P(A ∩ B) = P(A)P(B). If two events A and B are statistical independent, then the conditional probability equals the marginal probability: P(A|B) = P(A) and P(B|A) = P(B). The concept can be generalized to more than two events. The events A1, …, An are independent if and only if   P(⋂ni=1Ai)=∏ni=1P(Ai)\n",
    "\n",
    "\n",
    "### Law of total Probability\n",
    "\n",
    "In probability theory, the law (or formula) of total probability is a fundamental rule relating marginal probabilities to conditional probabilities. It expresses the total probability of an outcome which can be realized via several distinct events—hence the name.\n",
    "\n",
    "The law of total probability is a theorem that, in its discrete case, states if $ \\{B_{n}:n=1,2,3,\\ldots \\} $ is a finite or countably infinite partition of a sample space (in other words, a set of pairwise disjoint events whose union is the entire sample space) and each event $B_{n}$ is measurable, then for any event $A$ of the same probability space:\n",
    "\n",
    "$$P(A)=\\sum _{n}P(A\\cap B_{n})$$\n",
    "or, alternatively,\n",
    "\n",
    "$$ P(A)=\\sum _{n}P(A\\mid B_{n})P(B_{n}), $$\n",
    "\n",
    "where, for any $n$ for which $ P(B_{n})=0 $ these terms are simply omitted from the summation, because $P(A\\mid B_{n})$ is finite.\n",
    "\n",
    "The summation can be interpreted as a weighted average, and consequently the marginal probability, $P(A)$, is sometimes called \"average probability\"; \"overall probability\" is sometimes used in less formal writings.\n",
    "\n",
    "The law of total probability, can also be stated for conditional probabilities.\n",
    "\n",
    "$$ P(A\\mid C)=\\sum _{n}P(A\\mid C\\cap B_{n})P(B_{n}\\mid C)$$\n",
    "\n",
    "Taking the $B_{n}$ as above, and assuming $C$ is an event independent of any of the $B_{n}$:\n",
    "\n",
    "$$ P(A\\mid C)=\\sum _{n}P(A\\mid C\\cap B_{n})P(B_{n})$$\n",
    "\n",
    "The above mathematical statement might be interpreted as follows: given an event $A$, with known conditional probabilities given any of the $B_{n}$ events, each with a known probability itself, what is the total probability that $A$ will happen? The answer to this question is given by $P(A)$.\n",
    "\n",
    "### Chain Rule of Probability\n",
    "\n",
    "In probability theory, the chain rule permits the calculation of any member of the joint distribution of a set of random variables using only conditional probabilities. The rule is useful in the study of Bayesian networks, which describe a probability distribution in terms of conditional probabilities.\n",
    "\n",
    "The **Product Rule** states:\n",
    "\n",
    "$$ P(X \\cap Y) = P(X | Y) * P(Y)$$\n",
    "\n",
    "So the joint probability that both $X$ and $Y$ will occur is equal to the product of two terms:\n",
    "\n",
    "- Probability that event $Y$ occurs.\n",
    "- Probability that $X$ occurs given that $Y$ has already occurred.\n",
    "\n",
    "Generalizing the product rule leads to the **Chain Rule**. Let  $ A_1, A_2, \\dotsc, A_n $ be n events. The joint probability of all the n events is given by,\n",
    "\n",
    "$$ P(A_n \\cap \\dotsc \\cap A_1) = P(A_n | A_{n-1} \\cap \\dotsc \\cap A_1) * P(A_{n-1} \\cap \\dotsc \\cap A_1) $$\n",
    "\n",
    "which by induction can be turned into:\n",
    "\n",
    "$$ P(A_n \\cap \\dotsc \\cap A_1) = \\prod_{k=1}^{n} P(A_k | \\cap_{j=1}^{k-1} X_j  $$\n",
    "\n",
    "**Example:**\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    " P(A,B,C,D,E)  &= P(A|B,C,D,E) * P(B,C,D,E) \\\\ \n",
    "                 &= P(A|B,C,D,E) * P(B|C,D,E) * P(C,D,E) \\\\\n",
    "                 &= P(A|B,C,D,E) * P(B|C,D,E) * P(C | D,E) * P(D,E) \\\\\n",
    "                 &= P(A|B,C,D,E) * P(B|C,D,E) * P(C|D,E) * P(D|E) * P(E) \n",
    "\\end{aligned}                 \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability distributions\n",
    "\n",
    "In probability theory and statistics, a probability distribution is the mathematical function that gives the probabilities of occurrence of different possible outcomes for an experiment. It is a mathematical description of a random phenomenon in terms of its sample space and the probabilities of events (subsets of the sample space).\n",
    "\n",
    "For instance, if X is used to denote the outcome of a coin toss (\"the experiment\"), then the probability distribution of X would take the value 0.5 for X = heads, and 0.5 for X = tails (assuming that the coin is fair). Examples of random phenomena include the weather condition in a future date, the height of a person, the fraction of male students in a school, the results of a survey, etc.\n",
    "\n",
    "### Some Examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In probability theory, a normal (or Gaussian or Gauss or Laplace–Gauss) distribution is a type of continuous probability distribution for a real-valued random variable. The general form of its probability density function is\n",
    "\n",
    "$$ f(x)={\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2}}$$\n",
    "\n",
    "The parameter $\\mu$  is the mean or expectation of the distribution (and also its median and mode), while the parameter $\\sigma$  is its standard deviation. The variance of the distribution is $\\sigma ^{2}$. A random variable with a Gaussian distribution is said to be normally distributed, and is called a normal deviate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import seaborn as sns\n",
    "# generate random numbers from N(0,1)\n",
    "data_normal = norm.rvs(size=10000,loc=0,scale=1)\n",
    "\n",
    "ax = sns.distplot(data_normal,\n",
    "                  bins=100,\n",
    "                  kde=False,\n",
    "                  color='skyblue',\n",
    "                  hist_kws={\"linewidth\": 15,'alpha':1})\n",
    "ax.set(xlabel='Normal Distribution', ylabel='Frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Students Distribution\n",
    "In probability and statistics, Student's t-distribution (or simply the t-distribution) is any member of a family of continuous probability distributions that arise when estimating the mean of a normally-distributed population in situations where the sample size is small and the population's standard deviation is unknown. It was developed by English statistician William Sealy Gosset under the pseudonym \"Student\".\n",
    "\n",
    "The t-distribution plays a role in a number of widely used statistical analyses, including Student's t-test for assessing the statistical significance of the difference between two sample means, the construction of confidence intervals for the difference between two population means, and in linear regression analysis. The Student's t-distribution also arises in the Bayesian analysis of data from a normal family.\n",
    "\n",
    "If we take a sample of $n$ observations from a normal distribution, then the t-distribution with $\\nu=n-1$ degrees of freedom can be defined as the distribution of the location of the sample mean relative to the true mean, divided by the sample standard deviation, after multiplying by the standardizing term $\\sqrt {n}$. In this way, the t-distribution can be used to construct a confidence interval for the true mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "import seaborn as sns\n",
    "df = 2.74\n",
    "# generate random numbers from N(0,1)\n",
    "data_t = t.rvs(df, size=10000,loc=0,scale=1)\n",
    "\n",
    "ax = sns.distplot(data_t,\n",
    "                  bins=100,\n",
    "                  kde=False,\n",
    "                  color='skyblue',\n",
    "                  hist_kws={\"linewidth\": 15,'alpha':1})\n",
    "ax.set(xlabel='Students Distribution', ylabel='Frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binomial Distribution\n",
    "\n",
    "In probability theory and statistics, the binomial distribution with parameters n and p is the discrete probability distribution of the number of successes in a sequence of n independent experiments, each asking a yes–no question, and each with its own Boolean-valued outcome: success (with probability p) or failure (with probability q = 1 − p). A single success/failure experiment is also called a Bernoulli trial or Bernoulli experiment, and a sequence of outcomes is called a Bernoulli process; for a single trial, i.e., n = 1, the binomial distribution is a Bernoulli distribution. The binomial distribution is the basis for the popular binomial test of statistical significance.\n",
    "\n",
    "The binomial distribution is frequently used to model the number of successes in a sample of size n drawn with replacement from a population of size N. If the sampling is carried out without replacement, the draws are not independent and so the resulting distribution is a hypergeometric distribution, not a binomial one. However, for N much larger than n, the binomial distribution remains a good approximation, and is widely used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom\n",
    "import seaborn as sns\n",
    "data_binom = binom.rvs(n=10,p=0.8,size=10000)\n",
    "ax = sns.distplot(data_binom,\n",
    "                  kde=False,\n",
    "                  color='skyblue',\n",
    "                  hist_kws={\"linewidth\": 15,'alpha':1})\n",
    "ax.set(xlabel='Binomial Distribution', ylabel='Frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli distribution\n",
    "\n",
    "In probability theory and statistics, the Bernoulli distribution, named after Swiss mathematician Jacob Bernoulli, is the discrete probability distribution of a random variable which takes the value 1 with probability $p$ and the value 0 with probability $q=1-p$. Less formally, it can be thought of as a model for the set of possible outcomes of any single experiment that asks a yes–no question. Such questions lead to outcomes that are boolean-valued: a single bit whose value is success / yes / true / one with probability p and failure / no / false / zero with probability q. It can be used to represent a (possibly biased) coin toss where 1 and 0 would represent \"heads\" and \"tails\" (or vice versa), respectively, and p would be the probability of the coin landing on heads or tails, respectively. In particular, unfair coins would have $\\displaystyle p\\neq 1/2.$\n",
    "\n",
    "The Bernoulli distribution is a special case of the binomial distribution where a single trial is conducted (so n would be 1 for such a binomial distribution). It is also a special case of the two-point distribution, for which the possible outcomes need not be 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import bernoulli\n",
    "import seaborn as sns\n",
    "data_bern = bernoulli.rvs(size=10000,p=0.6)\n",
    "ax= sns.distplot(data_bern,\n",
    "                 kde=False,\n",
    "                 color=\"skyblue\",\n",
    "                 hist_kws={\"linewidth\": 15,'alpha':1})\n",
    "ax.set(xlabel='Bernoulli Distribution', ylabel='Frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson Distribution\n",
    "\n",
    "In probability theory and statistics, the Poisson distribution, named after French mathematician Siméon Denis Poisson, is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant mean rate and independently of the time since the last event. The Poisson distribution can also be used for the number of events in other specified intervals such as distance, area or volume.\n",
    "\n",
    "For instance, a call center receives an average of 180 calls per hour, 24 hours a day. The calls are independent; receiving one does not change the probability of when the next one will arrive. The number of calls received during any minute has a Poisson probability distribution: the most likely number is 3, but 2 and 4 are also likely and there is a small probability of it being as low as zero and a very small probability it could be 10. Another example is the number of decay events that occur from a radioactive source in a given observation period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import poisson\n",
    "import seaborn as sns\n",
    "data_poisson = poisson.rvs(mu=3, size=10000)\n",
    "ax = sns.distplot(data_poisson,\n",
    "                  bins=30,\n",
    "                  kde=False,\n",
    "                  color='skyblue',\n",
    "                  hist_kws={\"linewidth\": 15,'alpha':1})\n",
    "ax.set(xlabel='Poisson Distribution', ylabel='Frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Density Function\n",
    "\n",
    "In probability theory, a probability density function (PDF), or density of a continuous random variable, is a function whose value at any given sample (or point) in the sample space (the set of possible values taken by the random variable) can be interpreted as providing a relative likelihood that the value of the random variable would equal that sample. In other words, while the absolute likelihood for a continuous random variable to take on any particular value is 0 (since there are an infinite set of possible values to begin with), the value of the PDF at two different samples can be used to infer, in any particular draw of the random variable, how much more likely it is that the random variable would equal one sample compared to the other sample.\n",
    "\n",
    "In a more precise sense, the PDF is used to specify the probability of the random variable falling within a particular range of values, as opposed to taking on any one value. This probability is given by the integral of this variable's PDF over that range—that is, it is given by the area under the density function but above the horizontal axis and between the lowest and greatest values of the range. The probability density function is nonnegative everywhere, and its integral over the entire space is equal to 1.\n",
    "\n",
    "The terms \"probability distribution function\" and \"probability function\" have also sometimes been used to denote the probability density function. However, this use is not standard among probabilists and statisticians. In other sources, \"probability distribution function\" may be used when the probability distribution is defined as a function over general sets of values or it may refer to the cumulative distribution function, or it may be a probability mass function (PMF) rather than the density. \"Density function\" itself is also used for the probability mass function, leading to further confusion. In general though, the PMF is used in the context of discrete random variables (random variables that take values on a countable set), while the PDF is used in the context of continuous random variables.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Suppose bacteria of a certain species typically live 4 to 6 hours. The probability that a bacterium lives exactly 5 hours is equal to zero. A lot of bacteria live for approximately 5 hours, but there is no chance that any given bacterium dies at exactly 5.0000000000... hours. However, the probability that the bacterium dies between 5 hours and 5.01 hours is quantifiable. Suppose the answer is 0.02 (i.e., 2%). Then, the probability that the bacterium dies between 5 hours and 5.001 hours should be about 0.002, since this time interval is one-tenth as long as the previous. The probability that the bacterium dies between 5 hours and 5.0001 hours should be about 0.0002, and so on.\n",
    "\n",
    "In these three examples, the ratio (probability of dying during an interval) / (duration of the interval) is approximately constant, and equal to 2 per hour (or 2 hour−1). For example, there is 0.02 probability of dying in the 0.01-hour interval between 5 and 5.01 hours, and (0.02 probability / 0.01 hours) = 2 hour−1. This quantity 2 hour−1 is called the probability density for dying at around 5 hours. Therefore, the probability that the bacterium dies at 5 hours can be written as (2 hour−1) dt. This is the probability that the bacterium dies within an infinitesimal window of time around 5 hours, where dt is the duration of this window. For example, the probability that it lives longer than 5 hours, but shorter than (5 hours + 1 nanosecond), is (2 hour−1)×(1 nanosecond) ≈ 6×10−13 (using the unit conversion 3.6×1012 nanoseconds = 1 hour).\n",
    "\n",
    "There is a probability density function f with f(5 hours) = 2 hour−1. The integral of f over any window of time (not only infinitesimal windows but also large windows) is the probability that the bacterium dies in that window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
